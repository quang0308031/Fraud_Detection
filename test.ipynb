{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('C:\\\\Users\\\\Admin\\\\Downloads\\\\creditcard.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = np.array(data['Time'])\n",
    "delta_time = time[1:] - time[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Time'] = np.hstack((0, delta_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_point = np.array(data)[0: 50][np.newaxis, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 50, 31)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_point.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284802</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-11.881118</td>\n",
       "      <td>10.071785</td>\n",
       "      <td>-9.834783</td>\n",
       "      <td>-2.066656</td>\n",
       "      <td>-5.364473</td>\n",
       "      <td>-2.606837</td>\n",
       "      <td>-4.918215</td>\n",
       "      <td>7.305334</td>\n",
       "      <td>1.914428</td>\n",
       "      <td>...</td>\n",
       "      <td>0.213454</td>\n",
       "      <td>0.111864</td>\n",
       "      <td>1.014480</td>\n",
       "      <td>-0.509348</td>\n",
       "      <td>1.436807</td>\n",
       "      <td>0.250034</td>\n",
       "      <td>0.943651</td>\n",
       "      <td>0.823731</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284803</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.732789</td>\n",
       "      <td>-0.055080</td>\n",
       "      <td>2.035030</td>\n",
       "      <td>-0.738589</td>\n",
       "      <td>0.868229</td>\n",
       "      <td>1.058415</td>\n",
       "      <td>0.024330</td>\n",
       "      <td>0.294869</td>\n",
       "      <td>0.584800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.214205</td>\n",
       "      <td>0.924384</td>\n",
       "      <td>0.012463</td>\n",
       "      <td>-1.016226</td>\n",
       "      <td>-0.606624</td>\n",
       "      <td>-0.395255</td>\n",
       "      <td>0.068472</td>\n",
       "      <td>-0.053527</td>\n",
       "      <td>24.79</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284804</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.919565</td>\n",
       "      <td>-0.301254</td>\n",
       "      <td>-3.249640</td>\n",
       "      <td>-0.557828</td>\n",
       "      <td>2.630515</td>\n",
       "      <td>3.031260</td>\n",
       "      <td>-0.296827</td>\n",
       "      <td>0.708417</td>\n",
       "      <td>0.432454</td>\n",
       "      <td>...</td>\n",
       "      <td>0.232045</td>\n",
       "      <td>0.578229</td>\n",
       "      <td>-0.037501</td>\n",
       "      <td>0.640134</td>\n",
       "      <td>0.265745</td>\n",
       "      <td>-0.087371</td>\n",
       "      <td>0.004455</td>\n",
       "      <td>-0.026561</td>\n",
       "      <td>67.88</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284805</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.240440</td>\n",
       "      <td>0.530483</td>\n",
       "      <td>0.702510</td>\n",
       "      <td>0.689799</td>\n",
       "      <td>-0.377961</td>\n",
       "      <td>0.623708</td>\n",
       "      <td>-0.686180</td>\n",
       "      <td>0.679145</td>\n",
       "      <td>0.392087</td>\n",
       "      <td>...</td>\n",
       "      <td>0.265245</td>\n",
       "      <td>0.800049</td>\n",
       "      <td>-0.163298</td>\n",
       "      <td>0.123205</td>\n",
       "      <td>-0.569159</td>\n",
       "      <td>0.546668</td>\n",
       "      <td>0.108821</td>\n",
       "      <td>0.104533</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284806</th>\n",
       "      <td>4.0</td>\n",
       "      <td>-0.533413</td>\n",
       "      <td>-0.189733</td>\n",
       "      <td>0.703337</td>\n",
       "      <td>-0.506271</td>\n",
       "      <td>-0.012546</td>\n",
       "      <td>-0.649617</td>\n",
       "      <td>1.577006</td>\n",
       "      <td>-0.414650</td>\n",
       "      <td>0.486180</td>\n",
       "      <td>...</td>\n",
       "      <td>0.261057</td>\n",
       "      <td>0.643078</td>\n",
       "      <td>0.376777</td>\n",
       "      <td>0.008797</td>\n",
       "      <td>-0.473649</td>\n",
       "      <td>-0.818267</td>\n",
       "      <td>-0.002415</td>\n",
       "      <td>0.013649</td>\n",
       "      <td>217.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>284807 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Time         V1         V2        V3        V4        V5        V6  \\\n",
       "0        0.0  -1.359807  -0.072781  2.536347  1.378155 -0.338321  0.462388   \n",
       "1        0.0   1.191857   0.266151  0.166480  0.448154  0.060018 -0.082361   \n",
       "2        1.0  -1.358354  -1.340163  1.773209  0.379780 -0.503198  1.800499   \n",
       "3        0.0  -0.966272  -0.185226  1.792993 -0.863291 -0.010309  1.247203   \n",
       "4        1.0  -1.158233   0.877737  1.548718  0.403034 -0.407193  0.095921   \n",
       "...      ...        ...        ...       ...       ...       ...       ...   \n",
       "284802   1.0 -11.881118  10.071785 -9.834783 -2.066656 -5.364473 -2.606837   \n",
       "284803   1.0  -0.732789  -0.055080  2.035030 -0.738589  0.868229  1.058415   \n",
       "284804   1.0   1.919565  -0.301254 -3.249640 -0.557828  2.630515  3.031260   \n",
       "284805   0.0  -0.240440   0.530483  0.702510  0.689799 -0.377961  0.623708   \n",
       "284806   4.0  -0.533413  -0.189733  0.703337 -0.506271 -0.012546 -0.649617   \n",
       "\n",
       "              V7        V8        V9  ...       V21       V22       V23  \\\n",
       "0       0.239599  0.098698  0.363787  ... -0.018307  0.277838 -0.110474   \n",
       "1      -0.078803  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288   \n",
       "2       0.791461  0.247676 -1.514654  ...  0.247998  0.771679  0.909412   \n",
       "3       0.237609  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321   \n",
       "4       0.592941 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "284802 -4.918215  7.305334  1.914428  ...  0.213454  0.111864  1.014480   \n",
       "284803  0.024330  0.294869  0.584800  ...  0.214205  0.924384  0.012463   \n",
       "284804 -0.296827  0.708417  0.432454  ...  0.232045  0.578229 -0.037501   \n",
       "284805 -0.686180  0.679145  0.392087  ...  0.265245  0.800049 -0.163298   \n",
       "284806  1.577006 -0.414650  0.486180  ...  0.261057  0.643078  0.376777   \n",
       "\n",
       "             V24       V25       V26       V27       V28  Amount  Class  \n",
       "0       0.066928  0.128539 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1      -0.339846  0.167170  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2      -0.689281 -0.327642 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3      -1.175575  0.647376 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4       0.141267 -0.206010  0.502292  0.219422  0.215153   69.99      0  \n",
       "...          ...       ...       ...       ...       ...     ...    ...  \n",
       "284802 -0.509348  1.436807  0.250034  0.943651  0.823731    0.77      0  \n",
       "284803 -1.016226 -0.606624 -0.395255  0.068472 -0.053527   24.79      0  \n",
       "284804  0.640134  0.265745 -0.087371  0.004455 -0.026561   67.88      0  \n",
       "284805  0.123205 -0.569159  0.546668  0.108821  0.104533   10.00      0  \n",
       "284806  0.008797 -0.473649 -0.818267 -0.002415  0.013649  217.00      0  \n",
       "\n",
       "[284807 rows x 31 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(data, sq_len, n_features):\n",
    "    inputs = tf.zeros((len(data), 0, n_features))\n",
    "\n",
    "    for i in range(sq_len):\n",
    "        inputs = tf.concat((inputs, np.array(data.shift(-i))[:,np.newaxis,:]), axis = 1)\n",
    "\n",
    "    return inputs[:-sq_len+1,:,:-1], inputs[:-sq_len+1, -1, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, labels = data_generator(data, 50, 31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([284758, 50, 30])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([284758])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transactional_Extension(tf.keras.layers.Layer):\n",
    "    def __init__(self, per_step: int, k: int):\n",
    "        super(Transactional_Extension, self).__init__()\n",
    "        self.per_step = per_step\n",
    "        self.k = k\n",
    "    \n",
    "    def call(self, input):\n",
    "        pad = tf.tile(input[:, 0:1], [1, self.per_step - 1, 1])\n",
    "        fix_data = tf.concat((pad, input), axis = 1)\n",
    "        data_split = tf.convert_to_tensor([fix_data[:, i : i + self.per_step] for i in range(input.shape[1])])\n",
    "        data_cells = tf.transpose(tf.convert_to_tensor([data_split[i : i + self.k] for i in range(data_split.shape[0] - self.k + 1)]), perm=[2, 0, 1, 3, 4])\n",
    "        return data_cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(tf.keras.layers.Layer):\n",
    "    def __init__(self, k, w_aq, w_ah, b_a, v):\n",
    "        self.w_aq = w_aq\n",
    "        self.w_ah = w_ah\n",
    "        self.b_a = b_a\n",
    "        self.k = k\n",
    "        self.v = v\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "    def call(self, ht, ct, hi):\n",
    "        q = ht[:,tf.newaxis,...] + ct[:,tf.newaxis,...]\n",
    "        aq = self.w_aq(q)\n",
    "        ah = self.w_ah(hi[:, -self.k:])\n",
    "        oti = tf.math.tanh(aq + ah + self.b_a)\n",
    "        alpha = self.v(oti)\n",
    "        alpha_exp = tf.math.exp(alpha)\n",
    "        alphati = alpha_exp / tf.math.reduce_sum(alpha_exp)\n",
    "        e = tf.reduce_sum(tf.squeeze(tf.reduce_sum(alphati, axis=1)) * hi[:, -self.k:], axis=1)\n",
    "        e_exp = tf.math.exp(e)\n",
    "        return e_exp / tf.reduce_sum(e_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InteractionModule(tf.keras.layers.Layer):\n",
    "    def __init__(self, wh, we, wg, b_h):\n",
    "        self.wh = wh\n",
    "        self.we = we\n",
    "        self.wg = wg\n",
    "        self.b_h = b_h\n",
    "        super(InteractionModule, self).__init__()\n",
    "\n",
    "    def call(self, ht, e, g):\n",
    "        _h = self.wh(ht)\n",
    "        _e = self.we(e)\n",
    "        _g = self.wg(g)\n",
    "        new_h = tf.math.tanh(_h + _e + _g + self.b_h)\n",
    "        return new_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cell(tf.keras.layers.Layer):\n",
    "    def __init__(self, w_sh, w_sx, w_st, b_s, \n",
    "                 w_fh, w_fx, w_fs, b_f, \n",
    "                 w_ih, w_ix, w_is, b_i, \n",
    "                 w_Th, w_Tx, w_Ts, b_T, \n",
    "                 w_elph, w_elpx, w_elps, b_elp, \n",
    "                 w_oh, w_ox, w_os, b_o, \n",
    "                 w_aq, w_ah, b_a, \n",
    "                 k, v, wh, we, wg, b_h):\n",
    "        super(cell, self).__init__()\n",
    "        self.w_sh = w_sh\n",
    "        self.w_sx = w_sx\n",
    "        self.w_st = w_st\n",
    "        self.b_s = b_s\n",
    "\n",
    "        self.w_fh = w_fh\n",
    "        self.w_fx = w_fx\n",
    "        self.w_fs = w_fs\n",
    "        self.b_f = b_f\n",
    "\n",
    "        self.w_ih = w_ih\n",
    "        self.w_ix = w_ix\n",
    "        self.w_is = w_is\n",
    "        self.b_i = b_i\n",
    "\n",
    "        self.w_Th = w_Th\n",
    "        self.w_Tx = w_Tx\n",
    "        self.w_Ts = w_Ts\n",
    "        self.b_T = b_T\n",
    "\n",
    "        self.w_elph = w_elph\n",
    "        self.w_elpx = w_elpx\n",
    "        self.w_elps = w_elps\n",
    "        self.b_elp = b_elp\n",
    "\n",
    "        self.w_oh = w_oh\n",
    "        self.w_ox = w_ox\n",
    "        self.w_os = w_os\n",
    "        self.b_o = b_o\n",
    "\n",
    "        self.attn = Attention(k, w_aq, w_ah, b_a, v)\n",
    "        self.IM = InteractionModule(wh, we, wg, b_h)\n",
    "\n",
    "    def call(self, x_t, prev_h, prev_c, dt, store):\n",
    "        s_x = self.w_sx(x_t)\n",
    "        s_t = self.w_st(dt)\n",
    "        s_h = self.w_sh(prev_h)\n",
    "\n",
    "        s = tf.math.tanh(s_h + s_x + s_t[..., tf.newaxis] + self.b_s)\n",
    "\n",
    "        f_x = self.w_fx(x_t)\n",
    "        f_t = self.w_fs(s)\n",
    "        f_h = self.w_fh(prev_h)\n",
    "\n",
    "        f = tf.math.sigmoid(f_h + f_x + f_t + self.b_f)\n",
    "\n",
    "        i_x = self.w_ix(x_t)\n",
    "        i_t = self.w_is(s)\n",
    "        i_h = self.w_ih(prev_h)\n",
    "\n",
    "        i = tf.math.sigmoid(i_h + i_x + i_t + self.b_i)\n",
    "\n",
    "        T_x = self.w_Tx(x_t)\n",
    "        T_t = self.w_Ts(s)\n",
    "        T_h = self.w_Th(prev_h)\n",
    "\n",
    "        T = tf.math.sigmoid(T_h + T_x + T_t + self.b_T)\n",
    "\n",
    "        elp_x = self.w_elpx(x_t)\n",
    "        elp_t = self.w_elps(s)\n",
    "        elp_h = self.w_elph(prev_h)\n",
    "\n",
    "        elp = tf.math.tanh(elp_h + elp_x + elp_t + self.b_elp)\n",
    "\n",
    "        new_c = f * prev_c + i * elp + T * s\n",
    "\n",
    "        o_x = self.w_ox(x_t)\n",
    "        o_t = self.w_os(s)\n",
    "        o_h = self.w_oh(prev_h)\n",
    "\n",
    "        o = tf.math.sigmoid(o_h + o_x + o_t + self.b_o)\n",
    "\n",
    "        h = o * tf.math.sigmoid(new_c)\n",
    "\n",
    "        attn_score = self.attn(h, new_c, store)\n",
    "        print(attn_score.shape)\n",
    "        new_h = self.IM(h, attn_score, x_t)\n",
    "        return new_c, new_h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TH_LSTM(tf.keras.layers.Layer):\n",
    "    def __init__(self, len: int, per_step: int, k: int):\n",
    "        super(TH_LSTM, self).__init__()\n",
    "        self.units = len - k + 1\n",
    "        self.k = k\n",
    "        self.per_step = per_step\n",
    "        self.TE = Transactional_Extension(per_step = per_step, k = k)\n",
    "        self.norm = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        batch = input_shape[0]\n",
    "        input_shape = [self.TE.k, self.per_step, input_shape[-1]]\n",
    "        self.w_sh = tf.keras.layers.EinsumDense('...gxf,gxd->...gxd', output_shape=input_shape, bias_axes='xd', name='Time_aware_input_hidden_state')\n",
    "        self.w_sx = tf.keras.layers.EinsumDense('...gxf,gxd->...gxd', output_shape=input_shape, bias_axes='xd', name='Time_aware_input')\n",
    "        self.w_st = tf.keras.layers.EinsumDense('...xf,xd->...xd', output_shape=input_shape[:-1], bias_axes='xd', name='Time_aware_time_step')\n",
    "        self.b_s = self.add_weight('Time_aware_bias', shape = input_shape[1:], initializer='random_normal', trainable=True)\n",
    "\n",
    "        self.w_fh = tf.keras.layers.EinsumDense('...gxf,gxd->...gxd', output_shape=input_shape, bias_axes='xd', name='Forget_gate_hidden_state')\n",
    "        self.w_fx = tf.keras.layers.EinsumDense('...gxf,gxd->...gxd', output_shape=input_shape, bias_axes='xd', name='Forget_gate_input')\n",
    "        self.w_fs = tf.keras.layers.EinsumDense('...gxf,gxd->...gxd', output_shape=input_shape, bias_axes='xd', name='Forget_gate_time_stamp')\n",
    "        self.b_f = self.add_weight('Forget_gate_bias', shape = input_shape[1:], initializer='random_normal', trainable=True)\n",
    "\n",
    "        self.w_ih = tf.keras.layers.EinsumDense('...gxf,gxd->...gxd', output_shape=input_shape, bias_axes='xd', name='Input_gate_hidden_state')\n",
    "        self.w_ix = tf.keras.layers.EinsumDense('...gxf,gxd->...gxd', output_shape=input_shape, bias_axes='xd', name='Input_gate_input')\n",
    "        self.w_is = tf.keras.layers.EinsumDense('...gxf,gxd->...gxd', output_shape=input_shape, bias_axes='xd', name='Input_gate_time_stamp')\n",
    "        self.b_i = self.add_weight('Input_gate_bias', shape = input_shape[1:], initializer='random_normal', trainable=True)\n",
    "\n",
    "        self.w_Th = tf.keras.layers.EinsumDense('...gxf,gxd->...gxd', output_shape=input_shape, bias_axes='xd', name='Time-aware_gate_hidden_state')\n",
    "        self.w_Tx = tf.keras.layers.EinsumDense('...gxf,gxd->...gxd', output_shape=input_shape, bias_axes='xd', name='Time-aware_gate_input')\n",
    "        self.w_Ts = tf.keras.layers.EinsumDense('...gxf,gxd->...gxd', output_shape=input_shape, bias_axes='xd', name='Time-aware_gate_time_stamp')\n",
    "        self.b_T = self.add_weight('Time-aware_gate_bias', shape = input_shape[1:], initializer='random_normal', trainable=True)\n",
    "\n",
    "        self.w_elph = tf.keras.layers.EinsumDense('...gxf,gxd->...gxd', output_shape=input_shape, bias_axes='xd', name='Generated_candidate_cell_state_hidden_state')\n",
    "        self.w_elpx = tf.keras.layers.EinsumDense('...gxf,gxd->...gxd', output_shape=input_shape, bias_axes='xd', name='Generated_candidate_cell_state_input')\n",
    "        self.w_elps = tf.keras.layers.EinsumDense('...gxf,gxd->...gxd', output_shape=input_shape, bias_axes='xd', name='Generated_candidate_cell_state_time_stamp')\n",
    "        self.b_elp = self.add_weight('Generated_candidate_cell_state_bias', shape = input_shape, initializer='random_normal', trainable=True)\n",
    "\n",
    "        self.w_oh = tf.keras.layers.EinsumDense('...gxf,gxd->...gxd', output_shape=input_shape, bias_axes='xd', name='Output_gate_hidden_state')\n",
    "        self.w_ox = tf.keras.layers.EinsumDense('...gxf,gxd->...gxd', output_shape=input_shape, bias_axes='xd', name='Output_gate_input')\n",
    "        self.w_os = tf.keras.layers.EinsumDense('...gxf,gxd->...gxd', output_shape=input_shape, bias_axes='xd', name='Output_gate_time_stamp')\n",
    "        self.b_o = self.add_weight('Output_gate_bias', shape = input_shape[1:], initializer='random_normal', trainable=True)\n",
    "\n",
    "        self.w_aq = tf.keras.layers.EinsumDense('...gxf,gxd->...gxd', output_shape=input_shape, bias_axes='xd', name='aq')\n",
    "        self.w_ah = tf.keras.layers.EinsumDense('...gxf,gxd->...gxd', output_shape=input_shape, bias_axes='xd', name='ah')\n",
    "        self.b_a = self.add_weight('ba', shape = input_shape[1:], initializer='random_normal', trainable=True)\n",
    "\n",
    "        self.w_h = tf.keras.layers.EinsumDense('...gxf,gxd->...gxd', output_shape=input_shape, bias_axes='xd', name='wh')\n",
    "        self.w_e = tf.keras.layers.EinsumDense('...gxf,gxd->...gxd', output_shape=input_shape, bias_axes='xd', name='we')\n",
    "        self.w_g = tf.keras.layers.EinsumDense('...gxf,gxd->...gxd', output_shape=input_shape, bias_axes='xd', name='wg')\n",
    "        self.b_h = self.add_weight('ba', shape = input_shape[1:], initializer='random_normal', trainable=True)\n",
    "\n",
    "        self.v = tf.keras.layers.EinsumDense('...gxf,gxd->...gxd', output_shape=input_shape, bias_axes='xd', name='v')\n",
    "\n",
    "        self.cells = [cell(self.w_sh, self.w_sx, self.w_st, self.b_s, \n",
    "                           self.w_fh, self.w_fx, self.w_fs, self.b_f, \n",
    "                           self.w_ih, self.w_ix, self.w_is, self.b_i, \n",
    "                           self.w_Th, self.w_Tx, self.w_Ts, self.b_T, \n",
    "                           self.w_elph, self.w_elpx, self.w_elps, self.b_elp, \n",
    "                           self.w_oh, self.w_ox, self.w_os, self.b_o, \n",
    "                           self.w_aq, self.w_ah, self.b_a, \n",
    "                           self.k, self.v, self.w_h, self.w_e, self.w_g, self.b_h) \n",
    "                      for i in range(self.units)]\n",
    "        \n",
    "        self.h_init = self.add_weight('h_init', shape=[1] + input_shape, initializer='zeros', trainable=False)\n",
    "        self.c_init = self.add_weight('h_init', shape=[1] + input_shape, initializer='zeros', trainable=False)\n",
    "        return super().build(input_shape)\n",
    "    \n",
    "    def call(self, data_sq):\n",
    "        shape = tf.shape(data_sq)\n",
    "        str_mem = tf.zeros((shape[0], self.k, self.k, self.per_step, data_sq.shape[-1]))\n",
    "        print(str_mem)\n",
    "        data_split = self.TE(data_sq)\n",
    "        h = self.h_init\n",
    "        c = self.c_init\n",
    "        \n",
    "        for i in range(self.units):\n",
    "            c, h = self.cells[i](data_split[:, i,..., 1:], h, c, data_split[:, i,... , 0], str_mem)\n",
    "            str_mem = tf.concat((str_mem, h[:, tf.newaxis, ...]), axis = 1)\n",
    "        h = self.norm(h)\n",
    "        c = self.norm(c)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "TH = TH_LSTM(50, 20, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[[[0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    ...\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    ...\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    ...\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      "   ...\n",
      "\n",
      "   [[0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    ...\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    ...\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    ...\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]]]\n",
      "\n",
      "\n",
      "  [[[0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    ...\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    ...\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    ...\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      "   ...\n",
      "\n",
      "   [[0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    ...\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    ...\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    ...\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]]]\n",
      "\n",
      "\n",
      "  [[[0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    ...\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    ...\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    ...\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      "   ...\n",
      "\n",
      "   [[0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    ...\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    ...\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    ...\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]]]\n",
      "\n",
      "\n",
      "  ...\n",
      "\n",
      "\n",
      "  [[[0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    ...\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    ...\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    ...\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      "   ...\n",
      "\n",
      "   [[0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    ...\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    ...\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    ...\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]]]\n",
      "\n",
      "\n",
      "  [[[0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    ...\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    ...\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    ...\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      "   ...\n",
      "\n",
      "   [[0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    ...\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    ...\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    ...\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]]]\n",
      "\n",
      "\n",
      "  [[[0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    ...\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    ...\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    ...\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      "   ...\n",
      "\n",
      "   [[0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    ...\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    ...\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    ...\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]\n",
      "    [0. 0. 0. ... 0. 0. 0.]]]]], shape=(1, 10, 10, 20, 31), dtype=float32)\n",
      "(1, 10, 10, 20, 31)\n",
      "(1, 10, 20, 31)\n",
      "(1, 10, 20, 31)\n",
      "(1, 10, 10, 20, 31)\n",
      "(1, 10, 20, 31)\n",
      "(1, 10, 20, 31)\n",
      "(1, 10, 10, 20, 31)\n",
      "(1, 10, 20, 31)\n",
      "(1, 10, 20, 31)\n",
      "(1, 10, 10, 20, 31)\n",
      "(1, 10, 20, 31)\n",
      "(1, 10, 20, 31)\n",
      "(1, 10, 10, 20, 31)\n",
      "(1, 10, 20, 31)\n",
      "(1, 10, 20, 31)\n",
      "(1, 10, 10, 20, 31)\n",
      "(1, 10, 20, 31)\n",
      "(1, 10, 20, 31)\n",
      "(1, 10, 10, 20, 31)\n",
      "(1, 10, 20, 31)\n",
      "(1, 10, 20, 31)\n",
      "(1, 10, 10, 20, 31)\n",
      "(1, 10, 20, 31)\n",
      "(1, 10, 20, 31)\n",
      "(1, 10, 10, 20, 31)\n",
      "(1, 10, 20, 31)\n",
      "(1, 10, 20, 31)\n",
      "(1, 10, 10, 20, 31)\n",
      "(1, 10, 20, 31)\n",
      "(1, 10, 20, 31)\n",
      "(1, 10, 10, 20, 31)\n",
      "(1, 10, 20, 31)\n",
      "(1, 10, 20, 31)\n",
      "(1, 10, 10, 20, 31)\n",
      "(1, 10, 20, 31)\n",
      "(1, 10, 20, 31)\n",
      "(1, 10, 10, 20, 31)\n",
      "(1, 10, 20, 31)\n",
      "(1, 10, 20, 31)\n",
      "(1, 10, 10, 20, 31)\n",
      "(1, 10, 20, 31)\n",
      "(1, 10, 20, 31)\n",
      "(1, 10, 10, 20, 31)\n",
      "(1, 10, 20, 31)\n",
      "(1, 10, 20, 31)\n",
      "(1, 10, 10, 20, 31)\n",
      "(1, 10, 20, 31)\n",
      "(1, 10, 20, 31)\n",
      "(1, 10, 10, 20, 31)\n",
      "(1, 10, 20, 31)\n",
      "(1, 10, 20, 31)\n",
      "(1, 10, 10, 20, 31)\n",
      "(1, 10, 20, 31)\n",
      "(1, 10, 20, 31)\n",
      "(1, 10, 10, 20, 31)\n",
      "(1, 10, 20, 31)\n",
      "(1, 10, 20, 31)\n",
      "(1, 10, 10, 20, 31)\n",
      "(1, 10, 20, 31)\n",
      "(1, 10, 20, 31)\n",
      "(1, 10, 10, 20, 31)\n",
      "(1, 10, 20, 31)\n",
      "(1, 10, 20, 31)\n",
      "(1, 10, 10, 20, 31)\n",
      "(1, 10, 20, 31)\n",
      "(1, 10, 20, 31)\n",
      "(1, 10, 10, 20, 31)\n",
      "(1, 10, 20, 31)\n",
      "(1, 10, 20, 31)\n",
      "(1, 10, 10, 20, 31)\n",
      "(1, 10, 20, 31)\n",
      "(1, 10, 20, 31)\n",
      "(1, 10, 10, 20, 31)\n",
      "(1, 10, 20, 31)\n",
      "(1, 10, 20, 31)\n",
      "(1, 10, 10, 20, 31)\n",
      "(1, 10, 20, 31)\n",
      "(1, 10, 20, 31)\n",
      "(1, 10, 10, 20, 31)\n",
      "(1, 10, 20, 31)\n",
      "(1, 10, 20, 31)\n",
      "(1, 10, 10, 20, 31)\n",
      "(1, 10, 20, 31)\n",
      "(1, 10, 20, 31)\n",
      "(1, 10, 10, 20, 31)\n",
      "(1, 10, 20, 31)\n",
      "(1, 10, 20, 31)\n",
      "(1, 10, 10, 20, 31)\n",
      "(1, 10, 20, 31)\n",
      "(1, 10, 20, 31)\n",
      "(1, 10, 10, 20, 31)\n",
      "(1, 10, 20, 31)\n",
      "(1, 10, 20, 31)\n",
      "(1, 10, 10, 20, 31)\n",
      "(1, 10, 20, 31)\n",
      "(1, 10, 20, 31)\n",
      "(1, 10, 10, 20, 31)\n",
      "(1, 10, 20, 31)\n",
      "(1, 10, 20, 31)\n",
      "(1, 10, 10, 20, 31)\n",
      "(1, 10, 20, 31)\n",
      "(1, 10, 20, 31)\n",
      "(1, 10, 10, 20, 31)\n",
      "(1, 10, 20, 31)\n",
      "(1, 10, 20, 31)\n",
      "(1, 10, 10, 20, 31)\n",
      "(1, 10, 20, 31)\n",
      "(1, 10, 20, 31)\n",
      "(1, 10, 10, 20, 31)\n",
      "(1, 10, 20, 31)\n",
      "(1, 10, 20, 31)\n",
      "(1, 10, 10, 20, 31)\n",
      "(1, 10, 20, 31)\n",
      "(1, 10, 20, 31)\n",
      "(1, 10, 10, 20, 31)\n",
      "(1, 10, 20, 31)\n",
      "(1, 10, 20, 31)\n",
      "(1, 10, 10, 20, 31)\n",
      "(1, 10, 20, 31)\n",
      "(1, 10, 20, 31)\n",
      "(1, 10, 10, 20, 31)\n",
      "(1, 10, 20, 31)\n",
      "(1, 10, 20, 31)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 10, 20, 31), dtype=float32, numpy=\n",
       "array([[[[-0.97570467,  1.176692  ,  1.0733176 , ...,  1.1885358 ,\n",
       "          -0.9122394 ,  1.1905085 ],\n",
       "         [-1.0242919 ,  1.0716457 ,  1.1013155 , ..., -0.7113095 ,\n",
       "           0.38469014, -0.09864672],\n",
       "         [ 0.93249315, -1.0767227 ,  1.1403899 , ...,  0.35825592,\n",
       "          -1.145021  , -0.71911633],\n",
       "         ...,\n",
       "         [ 0.6110509 ,  1.4875286 , -1.0817502 , ...,  1.0255133 ,\n",
       "           1.4889672 , -0.8318598 ],\n",
       "         [ 1.2933362 ,  0.39893702, -0.56880236, ..., -1.5941602 ,\n",
       "           0.31802157,  0.47317508],\n",
       "         [-1.0979834 , -1.0902103 , -1.0932243 , ..., -1.095734  ,\n",
       "          -0.32377937, -1.1039878 ]],\n",
       "\n",
       "        [[-1.3263509 ,  0.578947  ,  0.5166446 , ..., -0.7971894 ,\n",
       "           0.6361059 , -0.673308  ],\n",
       "         [ 1.014041  ,  1.037879  ,  1.0133493 , ...,  0.7067518 ,\n",
       "          -1.2833288 ,  1.0228721 ],\n",
       "         [ 0.80001724, -0.8055412 ,  0.06684397, ...,  0.12931502,\n",
       "           0.01220575,  1.3295996 ],\n",
       "         ...,\n",
       "         [-0.46941695,  1.03619   , -1.6639059 , ..., -0.32149723,\n",
       "          -0.27760226, -0.9237427 ],\n",
       "         [-1.0864049 , -0.05138013, -0.89093244, ..., -0.9599923 ,\n",
       "           1.4419906 ,  0.505021  ],\n",
       "         [-1.1905822 ,  1.4310439 , -0.57697994, ..., -0.4536555 ,\n",
       "           1.3138673 ,  2.0333872 ]],\n",
       "\n",
       "        [[-0.7926526 , -1.0536973 , -0.95019275, ..., -0.9244449 ,\n",
       "          -0.8955757 , -0.75251305],\n",
       "         [-1.6689816 ,  0.8508946 , -0.05483369, ..., -0.14155076,\n",
       "          -0.7717744 , -0.5329801 ],\n",
       "         [ 0.30435666,  1.188394  ,  1.2806206 , ...,  0.672768  ,\n",
       "          -1.1308341 ,  0.35366207],\n",
       "         ...,\n",
       "         [ 0.9210913 ,  1.0003175 ,  0.87619376, ...,  1.0001844 ,\n",
       "           0.37149614,  0.9972139 ],\n",
       "         [-0.7895762 ,  1.6614615 ,  0.5050229 , ..., -0.7993753 ,\n",
       "          -0.9610463 , -0.52331173],\n",
       "         [-1.3109802 , -0.25916982,  0.96201926, ...,  1.1360586 ,\n",
       "           1.1224475 , -1.2513688 ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.95040697,  1.0810627 , -0.9223324 , ..., -0.9552806 ,\n",
       "          -0.44540405, -0.79444313],\n",
       "         [ 1.0471998 , -1.2232426 ,  1.1036122 , ..., -1.1498382 ,\n",
       "           0.42643526, -1.3210223 ],\n",
       "         [-1.1531924 , -1.2484548 ,  0.8496247 , ..., -0.7213476 ,\n",
       "           1.3317964 , -1.0180296 ],\n",
       "         ...,\n",
       "         [-1.1349715 , -1.0131314 , -1.0565915 , ..., -0.11776615,\n",
       "          -1.1349158 ,  1.2170871 ],\n",
       "         [ 1.3707472 ,  1.3076462 , -1.0791774 , ..., -1.1705    ,\n",
       "          -1.2984068 , -0.20005098],\n",
       "         [ 1.3668958 , -1.1559243 , -0.66979766, ...,  1.3567613 ,\n",
       "          -0.67826337, -1.0192326 ]],\n",
       "\n",
       "        [[ 1.2302108 , -1.1986238 ,  0.13149789, ...,  0.2691134 ,\n",
       "           1.2071038 , -0.07469227],\n",
       "         [ 0.50071895,  0.37906933,  0.49338135, ...,  1.4036388 ,\n",
       "          -1.2990061 ,  0.66822624],\n",
       "         [ 0.06646466, -0.00736045, -0.6637901 , ..., -1.3842643 ,\n",
       "           2.0586345 , -0.17440468],\n",
       "         ...,\n",
       "         [-0.63762677,  1.5475763 , -0.5179137 , ..., -1.2075665 ,\n",
       "          -0.30377728, -0.9816909 ],\n",
       "         [-0.2625543 ,  1.2372812 , -0.22191273, ...,  0.5776567 ,\n",
       "           0.04298164,  1.2719961 ],\n",
       "         [-0.4015122 , -0.9413045 , -0.939216  , ..., -0.0348911 ,\n",
       "          -0.6545351 , -0.30909938]],\n",
       "\n",
       "        [[ 1.1617272 ,  1.012485  ,  0.50738275, ..., -1.4073089 ,\n",
       "          -0.9962716 , -0.04457727],\n",
       "         [-1.3575329 , -1.1462798 , -1.2845837 , ...,  0.804892  ,\n",
       "           0.9077235 ,  0.70483744],\n",
       "         [ 0.78351194, -0.02780071,  1.6816807 , ..., -0.11540448,\n",
       "           0.77351475, -1.1517136 ],\n",
       "         ...,\n",
       "         [ 1.309732  ,  1.2372634 ,  1.5470878 , ..., -0.69793344,\n",
       "          -0.44411296, -0.68664235],\n",
       "         [ 0.87645143,  0.9477866 ,  0.93904465, ...,  0.8236411 ,\n",
       "           0.67325   ,  0.68251747],\n",
       "         [-1.0409306 , -0.25804207, -0.9452303 , ...,  0.2050595 ,\n",
       "          -0.75801575,  1.3991498 ]]]], dtype=float32)>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TH(test_point[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(TH_LSTM(50, 20, 10))\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50, 30]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(inputs.shape[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build(input_shape=[None] + list(inputs.shape[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " th_lstm (TH_LSTM)           (None, 10, 20, 30)        174460    \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 6000)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 6001      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 180461 (704.93 KB)\n",
      "Trainable params: 168461 (658.05 KB)\n",
      "Non-trainable params: 12000 (46.88 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(284758,), dtype=float32, numpy=array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    }
   ],
   "source": [
    "model.fit(x=inputs, y=labels, batch_size=64, epochs=100, shuffle=False, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
